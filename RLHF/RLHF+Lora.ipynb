{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading model from ðŸ¤—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jun 16 16:31:17 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 515.43.04    Driver Version: 515.43.04    CUDA Version: 11.7     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-DGXS...  On   | 00000000:07:00.0 Off |                    0 |\n",
      "| N/A   31C    P0    36W / 300W |     28MiB / 16384MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-DGXS...  On   | 00000000:08:00.0 Off |                    0 |\n",
      "| N/A   33C    P0    36W / 300W |      4MiB / 16384MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla V100-DGXS...  On   | 00000000:0E:00.0 Off |                    0 |\n",
      "| N/A   31C    P0    37W / 300W |      4MiB / 16384MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla V100-DGXS...  On   | 00000000:0F:00.0 Off |                    0 |\n",
      "| N/A   31C    P0    36W / 300W |      4MiB / 16384MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A     40642      G   /usr/lib/xorg/Xorg                  9MiB |\n",
      "|    0   N/A  N/A     40678      G   /usr/bin/gnome-shell               14MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append(\".\")\n",
    "from utils import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
      "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
      "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
      "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
      "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
      "    \n",
      "    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n",
      "    Setting a new token will erase the existing one.\n",
      "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved in your configured git credential helpers (store).\n",
      "Your token has been saved to /home/m_bobrin/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import interpreter_login\n",
    "\n",
    "interpreter_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import textwrap\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0, 1, 2\"\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "EVAL_PROMPTS = [\n",
    "    \"Your are an Pytorch expert. Question: How to create an attention layer in pytorch? \\n\\n Answer: \",\n",
    "    \"Question: I recieve TypeError with a following code: ```a=[1, 2, 3]    a.extend(4)```. How can I fix it?  \\n\\n Answer: \",\n",
    "    \"Question: What library would you recommend for visualizing 3D points in python? \\n\\n Answer: \",\n",
    "]\n",
    "\n",
    "def generate_eval(model: AutoModelForCausalLM, tokenizer: AutoTokenizer):\n",
    "    print(\"Starting Evaluation...\")\n",
    "    model = model.to(device)\n",
    "    for eval_prompt in EVAL_PROMPTS:\n",
    "        batch = tokenizer(eval_prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            output_tokens = model.generate(**batch, max_new_tokens=128, do_sample=True, temperature=0.5, top_p=0.75)\n",
    "\n",
    "        print(\"\\n\\n\", textwrap.fill(tokenizer.decode(output_tokens[0], skip_special_tokens=False)))\n",
    "        print(\"*\"*100)\n",
    "        \n",
    "model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-1.3B\", torch_dtype=torch.float16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-1.3B\", add_eos_token=False, add_bos_token=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Your are an Pytorch expert. Question: How to create an attention layer\n",
      "in pytorch?    Answer:   The attention layer is a function that takes\n",
      "a sequence of inputs and outputs a sequence of outputs. The output of\n",
      "the attention layer is a sequence of attention scores. The attention\n",
      "score is a scalar value that is a weighted sum of the input sequence.\n",
      "The attention score is computed by the following formula:\n",
      "attention_score = attention_layer(input_sequence, attention_weights)\n",
      "The attention layer takes a sequence of inputs and outputs a sequence\n",
      "of outputs. The output of the attention layer is a sequence of\n",
      "attention scores. The attention score is a scalar value that is a\n",
      "weighted sum of the input sequence\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Question: I recieve TypeError with a following code: ```a=[1, 2, 3]\n",
      "a.extend(4)```. How can I fix it?     Answer:   You can use the extend\n",
      "method of a list.  ```python a = [1, 2, 3] a.extend(4)  print(a) # [1,\n",
      "2, 3, 4] ```  ## 3.6.2  ```python a = [1, 2, 3] a.extend(4)  print(a)\n",
      "# [1, 2, 3, 4] ```  ## 3.6.3  ```python a = [1, 2, 3\n",
      "****************************************************************************************************\n",
      "\n",
      "\n",
      " Question: What library would you recommend for visualizing 3D points\n",
      "in python?    Answer:   I would recommend using the pyOpenGL library.\n",
      "A:  I would recommend using the pyOpenGL library.   A:  You can use\n",
      "pyOpenGL to create a 3D model of a point. import pyOpenGL  point =\n",
      "pyOpenGL.Point(100, 100)  # Draw the model\n",
      "gl.glDrawArrays(gl.GL_POINTS, 0, 1)  # Set the model to the modelview\n",
      "matrix gl.glMatrixMode(gl.GL_MODELVIEW) gl.glLoad\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "generate_eval(model, tokenizer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **GPT-Neo was not trained to be ChatBot**\n",
    "* **We can solve this by finetuning using PEFT & RLHF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
